# Henry Bot M3 â€“ Multi-Agent Intelligent Routing System

Repository: [https://github.com/estebmaister/henry_bot_M3](https://github.com/estebmaister/henry_bot_M3)

â¸»

## 1. Context and Objectives

**Henry Bot M3** was implemented to simulate a real enterprise scenario: a mid-sized SaaS company with overloaded support teams and a high volume of misrouted tickets across HR, IT Support, and Finance.

The core business problems targeted were:

- HR questions landing in IT or Finance queues
- Slower resolution times due to manual triage and re-routing
- Inconsistent, nonâ€“policy-aligned answers from human agents

### Project Objectives

The assignment required:

1. A **multi-agent orchestration system** with:
   - An **Orchestrator Agent** that classifies user intent (e.g., HR vs Tech vs Finance)
   - **Conditional routing** to specialized RAG agents per department
2. An implementation based on **LangChain** components (chains, retrievers, agents) rather than ad-hoc code.   
3. **Full workflow tracing** using **Langfuse** for observability and debugging.   
4. At least **three specialized RAG agents** with domain-specific document collections.
5. A clear explanation of **technical decisions**.
6. **Bonus**: An Evaluator Agent that scores responses (1â€“10) on relevance, completeness, and accuracy, integrated via Langfuse.

Henry Bot M3 satisfies all these requirements and is fully reproducible from the repository.

â¸»

## 2. System Overview

At a high level, the system works as follows:

1. A user query enters the **Multi-Agent Orchestrator**.
2. An **Intent Classifier** uses semantic similarity (Sentence Transformers) to classify the query into:
   - `hr`
   - `tech`
   - `finance`
3. Based on classification and confidence, the orchestrator **routes** the query to the corresponding **RAG agent**:
   - HR Assistant
   - IT Support Assistant
   - Finance Assistant
4. The chosen agent runs a **RAG pipeline**:
   - Retrieve relevant documents from a FAISS index
   - Generate an answer grounded on the retrieved docs
5. Optionally, a **Quality Evaluator Agent** scores the response.
6. **Langfuse** records the complete trace: intent classification, retrieval, answer generation, and evaluation.

This architecture closely follows modern **multi-agent** patterns for LLM applications, where multiple specialized agents collaborate under an orchestrator instead of a single all-purpose model.   

â¸»

## 3. Architecture

### 3.1 Diagram

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Multi-Agent Orchestrator              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Intent Classifier â”‚â”€â”€â”€â–¶â”‚           Router        â”‚ â”‚
â”‚  â”‚  (Semantic        â”‚    â”‚  (Conditional Logic &   â”‚ â”‚
â”‚  â”‚   Similarity)     â”‚    â”‚   Agent Selection)      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚                 â”‚
        â–¼                 â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HR Agent    â”‚  â”‚ Tech Agent  â”‚  â”‚ Finance Agent â”‚
â”‚ (RAG + LLM) â”‚  â”‚ (RAG + LLM) â”‚  â”‚ (RAG + LLM)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                 â”‚                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Quality         â”‚
              â”‚ Evaluator       â”‚
              â”‚ (Scoring &      â”‚
              â”‚  Assessment)    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Core Components
	â€¢	MultiAgentSystem (entrypoint with CLI interface)
	â€¢	Intent Classifier (semantic similarity using Sentence Transformers)
	â€¢	Department RAG Agents:
	â€¢	HR Assistant
	â€¢	IT Support Assistant
	â€¢	Finance Assistant
	â€¢	FAISS-based retrievers with persistent storage and caching
	â€¢	CachedFAISSRetriever (production-ready caching layer)
	â€¢	Quality Evaluator Agent with dimension scoring
	â€¢	Langfuse client for complete workflow tracing
	â€¢	Configuration management with environment variable support
	â€¢	CLI commands for system management and operations

### 3.3 Tech Stack
	â€¢	Language: Python 3.9+ (asyncio for concurrency)
	â€¢	LLM Orchestration: LangChain (chains, agents, retrievers, vectorstores)
	â€¢	Vector Store: FAISS for similarity search over embeddings
	â€¢	Embeddings: all-MiniLM-L6-v2 (Sentence Transformers)
	â€¢	LLM Provider: OpenRouter (configurable via environment)
	â€¢	Observability: Langfuse (traces, evals, metrics)

â¸»

## 4. Implementation Details

### 4.1 Project Structure

The repo is organized for clarity and maintainability:
```text
src/
â”œâ”€â”€ main.py                     # System entry point and CLI interface
â”œâ”€â”€ config.py                   # Configuration management with env support
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ base.py                 # Base agent interface
â”‚   â””â”€â”€ rag_agent.py            # RAG agent implementation
â”œâ”€â”€ retrievers/
â”‚   â”œâ”€â”€ base.py                 # Retriever interface
â”‚   â”œâ”€â”€ faiss_retriever.py      # FAISS-based retriever
â”‚   â””â”€â”€ cached_faiss_retriever.py # Cached FAISS implementation
â”œâ”€â”€ orchestrator/
â”‚   â”œâ”€â”€ intent_classifier.py    # Semantic intent classification
â”‚   â””â”€â”€ orchestrator.py         # Routing & coordination
â”œâ”€â”€ evaluator/
â”‚   â””â”€â”€ quality_evaluator.py    # Automated quality scoring
â””â”€â”€ utils/
    â”œâ”€â”€ langfuse_client.py      # Langfuse observability integration
    â””â”€â”€ mock_llm.py           # Mock LLM for testing

data/                           # Domain-specific documentation
â”œâ”€â”€ hr_docs/                   # HR policies and benefits
â”‚   â”œâ”€â”€ employee_benefits.md
â”‚   â””â”€â”€ workplace_policies.md
â”œâ”€â”€ tech_docs/                 # IT support and development
â”‚   â”œâ”€â”€ it_support.md
â”‚   â””â”€â”€ software_development.md
â””â”€â”€ finance_docs/              # Financial procedures
    â”œâ”€â”€ expenses_and_budgets.md
    â””â”€â”€ financial_policies.md

cache/                         # Runtime cache directory
store/                         # Persistent FAISS indices storage
â”œâ”€â”€ faiss_indices/            # FAISS index files per department
â”œâ”€â”€ embeddings/               # Pre-computed embeddings
â””â”€â”€ metadata/                 # Document metadata

tests/                          # Test suite
â””â”€â”€ test_integration.py        # Integration tests

test_queries.json              # Test queries in proper JSON format
.env.example                  # Environment variables template
```

This separation allows to evolve each layer (classification, retrieval, evaluation, observability) without touching core orchestration logic.

â¸»

## 5. Intent Classification & Routing

### 5.1 Semantic Intent Classifier

The intent classifier uses Sentence Transformers to encode:
	â€¢	Department â€œprototypeâ€ descriptions (HR, Tech, Finance)
	â€¢	Incoming user queries

The bot computes cosine similarity between the query embedding and each department prototype, then pick the department with the highest score.

Key aspects:
	â€¢	Configurable similarity top-k and confidence thresholds:
	â€¢	SIMILARITY_TOP_K (default: 3)
	â€¢	CONFIDENCE_THRESHOLD (0.7 for dev, 0.8+ for prod)
	â€¢	Output includes:
	â€¢	department
	â€¢	classification_confidence

### 5.2 Conditional Routing

Once the intent is classified, the orchestrator performs conditional routing:
	â€¢	hr      â†’ HR Assistant (RAG agent)
	â€¢	tech    â†’ IT Support Assistant
	â€¢	finance â†’ Finance Assistant

If the confidence is below the configured threshold, the bot can:
	â€¢	Route to a fallback agent (future extension), or
	â€¢	Ask for clarification / flag for human review

In the example run:
```bash
python3 -m src.main query --query "What benefits am I entitled to as a new employee?"
```

The system routes the query to the HR Assistant with logged confidence and processing time, and produces a structured HR benefits answer grounded in HR docs.

â¸»

## 6. RAG Agents & Retrieval

### 6.1 Per-Department RAG Agents

Each department agent follows the same pattern:
	1.	Retriever
	â€¢	FAISS-based vector store per department:
	â€¢	store/faiss_indices/<department>/faiss.index
	â€¢	store/embeddings/<department>/embeddings.npy
	â€¢	Built using all-MiniLM-L6-v2 embeddings.
	2.	Prompt Template
	â€¢	Department-specific system prompt:
	â€¢	Use only provided context.
	â€¢	If the answer is not in the docs, say so explicitly.
	â€¢	Use department terminology and policy tone.
	3.	LLM Chain
	â€¢	A LangChain RAG-style chain that:
	â€¢	Receives the user query
	â€¢	Calls the retriever (top_k configurable)
	â€¢	Injects retrieved chunks into the prompt
	â€¢	Calls the LLM to generate a grounded answer

### 6.2 Example Answer (HR Assistant)

For the benefits query, the HR agent:
	â€¢	Retrieves the employee benefits and PTO policy docs.
	â€¢	Produces a structured markdown answer including:
	â€¢	Health Insurance
	â€¢	PTO
	â€¢	Holidays
	â€¢	Retirement Benefits
	â€¢	Compensation
	â€¢	Training & Development
	â€¢	Remote Work
	â€¢	Required Training

The console output includes:
	â€¢	Department: hr
	â€¢	Agent: HR Assistant
	â€¢	Confidence: 0.579
	â€¢	Processing Time: ~11s
	â€¢	Quality Score: 9.3/10 (from the evaluator)

### 6.3 Persistent Storage & Caching System

A key production feature is the dual-layer storage system:

#### Persistent Storage (./store/)
- **FAISS Indices**: `store/faiss_indices/<dept>/faiss.index` - Pre-built vector indices per department
- **Embeddings**: `store/embeddings/<dept>/embeddings.npy` - Pre-computed document embeddings
- **Metadata**: `store/metadata/<dept>/` - Document metadata and chunk information
- **Benefit**: Instant system startup without reprocessing documents

#### Runtime Cache (./cache/)
- **In-Memory Caching**: `CachedFAISSRetriever` provides fast access to frequently accessed data
- **Cache Management**: CLI commands for cache inspection and clearing
- **Department-Specific**: Can clear cache for individual departments or entire system

#### Storage Management Commands
```bash
# View persistent store status
python3 -m src.main store-info

# Clear persistent data
python3 -m src.main store-clear --department hr

# Cache management
python3 -m src.main cache-info
python3 -m src.main cache-clear
```

This dual approach ensures both **fast startup** (persistent storage) and **runtime performance** (caching).

â¸»

## 7. Observability with Langfuse

### 7.1 Why Langfuse

Given the complexity of multi-step LLM workflows, I wanted deep observability: traces of each decision, retrieval call, and model output. Langfuse is an open-source LLM engineering platform that provides tracing, evaluations, metrics, and prompt management tailored to LLM apps.

### 7.2 Tracing Model

For each query, the bot creates a Langfuse trace:
	â€¢	Trace name: multi_agent_query_processing
	â€¢	Events:
	â€¢	intent_classification
	â€¢	Spans:
	â€¢	rag_retrieval
	â€¢	<Department> Assistant_execution
	â€¢	Metadata:
	â€¢	department
	â€¢	classification confidence
	â€¢	processing time
	â€¢	quality score (if evaluated)

Sample log (simplified):
```text
ğŸ“ [Langfuse] Creating trace: multi_agent_query_processing
ğŸ“… [Langfuse] Creating event: intent_classification
ğŸ“Š [Langfuse] Creating span: rag_retrieval
ğŸ“Š [Langfuse] Creating span: HR Assistant_execution
ğŸ”„ [Langfuse] Updating trace with: ['output', 'metadata']
```

This makes it easy to debug:
	â€¢	Misclassifications (wrong department)
	â€¢	Poor retrieval (irrelevant docs)
	â€¢	Hallucinations or low-quality answers

### 7.3 Tracing example

![langfuse example][./docs/langfuse.png]

â¸»

## 8. Quality Evaluation (Bonus)

### 8.1 Evaluator Agent

As a bonus, I added an Evaluator Agent that runs after the main RAG answer is generated.

It takes as input:
	â€¢	Original user query
	â€¢	Final answer
	â€¢	(Optionally) retrieved context

It outputs:
	â€¢	overall_score (1â€“10)
	â€¢	dimension_scores for:
	â€¢	relevance
	â€¢	completeness
	â€¢	accuracy
	â€¢	reasoning
	â€¢	recommendations

These scores are attached to the same Langfuse trace, so I can analyze quality over time, per department, or per query type.

### 8.2 Example Evaluation

For the benefits query, the evaluator produced:
	â€¢	Overall Score: 9.3/10
	â€¢	Dimension Scores:
	â€¢	relevance: 9/10
	â€¢	completeness: 10/10
	â€¢	accuracy: 9/10

This gives me an automated, consistent signal about response quality, and a foundation for future human-in-the-loop review workflows.

â¸»

## 9. Usage

The system provides a comprehensive CLI interface for production management:

### Core Commands
```bash
# Initialize the system (load indices, warm up agents)
python3 -m src.main init

# Initialize with force rebuild (useful after document updates)
python3 -m src.main init --force-rebuild

# Initialize without persistent storage (cache-only mode)
python3 -m src.main init --no-persistent

# Process a single query
python3 -m src.main query --query "What benefits am I entitled to as a new employee?"

# Query with user ID for better tracing
python3 -m src.main query --query "How do I reset my password?" --user-id "john.doe"

# Query without quality evaluation (faster processing)
python3 -m src.main query --query "What's our budget?" --no-evaluation

# Run a test suite of queries
python3 -m src.main test --file test_queries.json

# Run tests without quality evaluation (faster)
python3 -m src.main test --file test_queries.json --no-evaluation

# Check system status and component health
python3 -m src.main status
```

### Storage Management Commands
```bash
# View persistent store information
python3 -m src.main store-info

# Clear all persistent stores
python3 -m src.main store-clear

# Clear specific department store
python3 -m src.main store-clear --department hr

# View cache information
python3 -m src.main cache-info

# Clear all caches
python3 -m src.main cache-clear

# Clear specific department cache
python3 -m src.main cache-clear --department tech
```

### Available Flags
- `--force-rebuild`: Force rebuild all FAISS indices
- `--no-persistent`: Disable persistent storage (cache-only mode)
- `--no-evaluation`: Skip quality evaluation for faster processing
- `--user-id <id>`: User ID for better Langfuse tracing
- `--department <dept>`: Target department for cache/store operations (hr, tech, finance)
- `--file <path>`: Test queries file (default: test_queries.json)

â¸»

## 10. Results & Metrics

Latest Test Results (from 19 test queries)
	â€¢	Classification Accuracy: 68.4%
	â€¢	Average Confidence: 0.44
	â€¢	Average Processing Time: 4.9s
	â€¢	Average Quality Score: 7.3/10
	â€¢	Total Test Queries: 19 (7 HR, 8 Tech, 4 Finance)

These numbers are based on a comprehensive test set of 19 real-world HR, Tech, and Finance queries. The classification accuracy reflects the current intent classifier performance with semantic similarity, while the quality scores demonstrate strong response generation despite moderate classification confidence.

â¸»

## 11. Technical Decisions

### 11.1 Why LangChain

I chose LangChain instead of custom orchestration for several reasons:

	â€¢	Recommendation from instructor to try a state of the art framework
	â€¢	Ready-made abstractions for chains, agents, and retrievers
	â€¢	Cleaner composition of multi-step workflows
	â€¢	Easier to swap models, retrievers, and tools as the system grows

### 11.2 Why FAISS + Sentence Transformers
	â€¢	FAISS gives me fast similarity search over dense vectors, suitable for mid-sized documentation.
	â€¢	all-MiniLM-L6-v2 offers compact, high-quality sentence embeddings with good performance/latency trade-offs.
	â€¢	Together they provide robust semantic retrieval without overcomplicating the infrastructure.

### 11.3 Why Semantic Classification (Not Rules)
	â€¢	Handles paraphrases and non-standard phrasing better than keyword rules.
	â€¢	Easy to extend: to add a department, I just add more prototype examples and documents.
	â€¢	Produces confidence scores, which I can use to control routing and escalation logic.

### 11.4 Why Langfuse for Observability
	â€¢	Purpose-built for LLM observability, tracing and evaluation, not just generic logging.
	â€¢	Direct integration with Python and LangChain.
	â€¢	Gives me a clear view of each query's journey through the multi-agent pipeline.

### 11.5 Why Persistent Storage & Caching
	â€¢	Production Performance: Instant system startup with pre-built FAISS indices and embeddings
	â€¢	Scalability: Store large document collections without memory constraints
	â€¢	Reliability: Persistent indices survive system restarts and crashes
	â€¢	Development Efficiency: Skip document reprocessing during development and testing
	â€¢	Operational Management: CLI commands for cache/store inspection and maintenance

â¸»

## 12. Limitations & Future Work

Current limitations:
	â€¢	Only three departments implemented (HR, Tech, Finance).
	â€¢	Small document sets per department (good for demo, but not production-sized).
	â€¢	No explicit escalation to human agents yet when confidence or quality is low.

Planned improvements:
	â€¢	Add Legal and additional specialized agents.
	â€¢	Introduce a fallback â€œgeneralistâ€ agent for ambiguous queries.
	â€¢	Integrate with a real ticketing system (e.g., Zendesk/Freshdesk) for end-to-end routing.
	â€¢	Expand test datasets and add continuous evaluation pipelines.

â¸»

## 13. Conclusion

Henry Bot M3 is my implementation of a multi-agent intelligent routing system that:
	â€¢	Classifies user intent with semantic similarity
	â€¢	Routes queries to specialized, department-specific RAG agents
	â€¢	Grounds answers in internal documentation
	â€¢	Provides full observability and automated quality evaluation via Langfuse
	â€¢	Follows LangChain-based, production-ready patterns instead of fragile one-off scripts